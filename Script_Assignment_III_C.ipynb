{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJqomiTPPDeaILs4ebZxBN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#@title Step 2:Installing Hugging Face Transformers\n","\n","# Decided to Keep Tensorflow - feel free to delete if it gives any issues\n","\n","# Install 'transformers' from master\n","# !pip install git+https://github.com/huggingface/transformers\n","# !pip install git+https://github.com/huggingface/accelerate\n","# !pip list | grep -E 'transformers|tokenizers'\n","# transformers version at notebook update --- 2.9.1\n","# tokenizers version at notebook update --- 0.7.0\n","!pip install -U transformers\n","!pip install -U accelerate\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkDivjSw8uqI","executionInfo":{"status":"ok","timestamp":1709078791016,"user_tz":300,"elapsed":18952,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"071459e3-55d6-4b2e-eaed-d4ebb6cb3e38"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.39.0.dev0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.28.0.dev0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.9.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["#@ Main Imports for functionality\n","import os\n","from pathlib import Path\n","\n","from tokenizers import ByteLevelBPETokenizer\n","from tokenizers.implementations import ByteLevelBPETokenizer\n","from tokenizers.processors import BertProcessing\n","\n","import transformers\n","import accelerate\n","\n","from transformers import Trainer, TrainingArguments\n","from transformers import pipeline\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import LineByLineTextDataset\n","from transformers import RobertaForMaskedLM\n","from transformers import RobertaTokenizer\n","from transformers import RobertaConfig\n"],"metadata":{"id":"t-_rtnQQ8Ha2","executionInfo":{"status":"ok","timestamp":1709078796184,"user_tz":300,"elapsed":5170,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#@title Step 3: Training a Tokenizer\n","%%time\n","\n","paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n","# Initialize a tokenizer\n","tokenizer = ByteLevelBPETokenizer()\n","# Customize training\n","tokenizer.train(files=paths, vocab_size=52_000,\n","min_frequency=2, special_tokens=[\n","\"<s>\",\n","\"<pad>\",\n","\"</s>\",\n","\"<unk>\",\n","\"<mask>\",\n","])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBmKHzKHI7VE","executionInfo":{"status":"ok","timestamp":1709078801252,"user_tz":300,"elapsed":5078,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"41319276-fbee-4aab-8567-cca684b15e34"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 5.74 s, sys: 251 ms, total: 5.99 s\n","Wall time: 5.11 s\n"]}]},{"cell_type":"code","source":["#@title Step 4: Saving the files to disk\n","\n","token_dir = '/content/KantaiBERT'\n","if not os.path.exists(token_dir):\n","  os.makedirs(token_dir)\n","tokenizer.save_model('KantaiBERT')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mnPgz_EAIyLe","executionInfo":{"status":"ok","timestamp":1709078801253,"user_tz":300,"elapsed":16,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"37c2bc92-7c8c-4dd4-8601-fbcfadc6bacf"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['KantaiBERT/vocab.json', 'KantaiBERT/merges.txt']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#@title Step 5 Loading the Trained Tokenizer Files\n","tokenizer = ByteLevelBPETokenizer(\"./KantaiBERT/vocab.json\", \"./KantaiBERT/merges.txt\" )\n","tokenizer.encode(\"The Critique of Pure Reason.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Iavn-5kIyHg","executionInfo":{"status":"ok","timestamp":1709078801444,"user_tz":300,"elapsed":204,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"0dc0fb15-84b4-4235-d377-bd2e53745379"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Encoding(num_tokens=6, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#@title Step 7: Defining the configuration of the Model\n","\n","Original Configuration\n","config = RobertaConfig(\n","vocab_size=52_000,\n","max_position_embeddings=514,\n","num_attention_heads=12,\n","num_hidden_layers=6,\n","type_vocab_size=1,\n",")"],"metadata":{"id":"XorudcvWJPn-","executionInfo":{"status":"ok","timestamp":1709079115532,"user_tz":300,"elapsed":137,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#@title Step 8: Re-creating the Tokenizer in Transformers\n","tokenizer = RobertaTokenizer.from_pretrained(\"./KantaiBERT\", max_length=512)"],"metadata":{"id":"8oFNxhPWJW_3","executionInfo":{"status":"ok","timestamp":1709079117309,"user_tz":300,"elapsed":134,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#@title Step 9: Initializing a Model From Scratch\n","\n","model = RobertaForMaskedLM(config=config)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuoFhibdJd2g","executionInfo":{"status":"ok","timestamp":1709079120055,"user_tz":300,"elapsed":831,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"7148e030-1f5a-4291-82f5-1db4636c1f3e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["RobertaForMaskedLM(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(26000, 768, padding_idx=1)\n","      (position_embeddings): Embedding(257, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0-2): 3 x RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (lm_head): RobertaLMHead(\n","    (dense): Linear(in_features=768, out_features=768, bias=True)\n","    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","    (decoder): Linear(in_features=768, out_features=26000, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["#@title Step 10: Building the Dataset\n","%%time\n","dataset = LineByLineTextDataset(\n","tokenizer=tokenizer,\n","file_path=\"./kant.txt\",\n","block_size=128,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gQcQB_KJmLX","executionInfo":{"status":"ok","timestamp":1709079141920,"user_tz":300,"elapsed":16732,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"ca4840b3-ae52-4e40-df3c-da98aecb7a01"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["CPU times: user 16.3 s, sys: 146 ms, total: 16.4 s\n","Wall time: 16.6 s\n"]}]},{"cell_type":"code","source":["#@title Step 11: Defining a Data Collator\n","data_collator = DataCollatorForLanguageModeling(\n","tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"metadata":{"id":"EkQq-bDuJp5K","executionInfo":{"status":"ok","timestamp":1709079145571,"user_tz":300,"elapsed":120,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#@title Step 12: Initializing the Trainer\n","\n","training_args = TrainingArguments(\n","output_dir=\"./KantaiBERT\",\n","overwrite_output_dir=True,\n","num_train_epochs=1,\n","per_device_train_batch_size=64,\n","save_steps=10_000,\n","save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","model=model,\n","args=training_args,\n","data_collator=data_collator,\n","train_dataset=dataset,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXgTYY6SJto7","executionInfo":{"status":"ok","timestamp":1709079147307,"user_tz":300,"elapsed":125,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"a7dd7475-410a-4caa-bacf-b6f46078746a"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["#@title Step 13: Pre-training the Model\n","%%time\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":474},"id":"eZZy3SiwJyE-","executionInfo":{"status":"ok","timestamp":1709079159487,"user_tz":300,"elapsed":8829,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"6ab13457-8299-4ef4-9486-cc569ac8370b"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2' max='2672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   2/2672 : < :, Epoch 0.00/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1625\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1628\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1964\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2912\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2914\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1998\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1999\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#@title Step 14: Saving the Final Model(+tokenizer + config) to\n","disk\n","trainer.save_model(\"./KantaiBERT\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":179},"id":"EgRwNnW1J3iX","executionInfo":{"status":"error","timestamp":1709078987799,"user_tz":300,"elapsed":6,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}},"outputId":"e933bb22-9527-44c2-cd18-1ed1d09f6d5b"},"execution_count":14,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'disk' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-30a46f03ac7a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title Step 14: Saving the Final Model(+tokenizer + config) to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./KantaiBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'disk' is not defined"]}]},{"cell_type":"code","source":["#@title Step 15: Language Modeling with\n","the FillMaskPipeline\n","fill_mask = pipeline(\n","\"fill-mask\",\n","model=\"./KantaiBERT\",\n","tokenizer=\"./KantaiBERT\"\n",")"],"metadata":{"id":"2g4FW9xRJ8tL","executionInfo":{"status":"aborted","timestamp":1709078987800,"user_tz":300,"elapsed":6,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fill_mask(\"Human thinking involves human <mask>.\")"],"metadata":{"id":"4x05ki0xKCSS","executionInfo":{"status":"aborted","timestamp":1709078987800,"user_tz":300,"elapsed":5,"user":{"displayName":"Wessley Dennis","userId":"06736961053167719778"}}},"execution_count":null,"outputs":[]}]}